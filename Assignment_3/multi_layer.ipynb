{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRFuANsKQmKv"
   },
   "outputs": [],
   "source": [
    "%pycat run_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZBgiesB9QsBW",
    "outputId": "c8222026-235e-4b24-9e42-714e366d6153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_tests.py\n",
    "\n",
    "\n",
    "__author__ = \"Xenia Ioannidou\"\n",
    "\n",
    "from k_layer_network import NeuralNetwork\n",
    "from preprocessor import *\n",
    "import numpy as np\n",
    "from build_tests import *\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    tester = Tests()\n",
    "\n",
    "    # EXERCISE 1a: Checking gradients for 3-layers network\n",
    "    # tester.exercise_1a()\n",
    "\n",
    "    # EXERCISE 1b: Checking gradients for 9-layers network\n",
    "    # tester.exercise_1b()\n",
    "    # *********************************************************\n",
    "\n",
    "    # EXERCISE 2: Evolution of the loss for the 3-layer network\n",
    "    # tester.exercise_2()\n",
    "\n",
    "    # EXERCISE 3: Evolution of the loss for the 6-layer network\n",
    "    # tester.exercise_3()\n",
    "    # *********************************************************\n",
    "\n",
    "    # EXERCISE 4: Coarse search for lambda value in 3-layer network\n",
    "    # tester.coarse_search()\n",
    "    # *********************************************************\n",
    "\n",
    "    # EXERCISE 5: Fine search for lambda value in 3-layer network\n",
    "    # tester.fine_search()\n",
    "    # *********************************************************\n",
    "\n",
    "    # EXERCISE 6: Sensitivity to initialization in 3-layer network\n",
    "    # tester.check_sensitivity(True, 1e-1, \"BN_1e1\")\n",
    "    # tester.check_sensitivity(False, 1e-1,\"noBN_1e1\")\n",
    "    # tester.check_sensitivity(True, 1e-3, \"BN_1e3\")\n",
    "    # tester.check_sensitivity(False, 1e-3,\"noBN_1e3\")\n",
    "    tester.check_sensitivity(True, 1e-5, \"BN_1e5\")\n",
    "    tester.check_sensitivity(False, 1e-5,\"noBN_1e5\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b30_LPXyb9mL"
   },
   "outputs": [],
   "source": [
    "%pycat preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7lcHM3jYcarO",
    "outputId": "f5699a41-dfdc-4405-a98c-1144c33cfe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessor.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "__author__ = \"Xenia Ioannidou\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "num_of_classes = 10\n",
    "directory = \"//content//drive//My Drive//Colab_Deep_Learning_Lab3/\"\n",
    "file_labels = \"//content//drive//My Drive//Colab_Deep_Learning_Lab3//batches.meta\"\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "\n",
    "  def load_batch(self, directory, extension):\n",
    "      \"\"\"\n",
    "          dataset:\n",
    "              60.000 32x32 colour images in 10 classes, with 6.000 images per class\n",
    "\n",
    "              5 training batches: 1 batch has 10.000 images, so 50.000 training images and\n",
    "              5 training batches: 5000 images from each class, 10 classes, so 50.000 training images\n",
    "              1 test batch: 10.000 test images\n",
    "              1 image is 32x32x3\n",
    "\n",
    "          labels:\n",
    "              a list of 10.000 numbers in the range 0-9\n",
    "              the number in position i is the label of the i_th image in the array data\n",
    "\n",
    "          A.data:\n",
    "              an array 10.000 rows and 3072 columns (32x32x3), where 1 image is one row in A.data\n",
    "\n",
    "          return:\n",
    "              the image and label data in separate files\n",
    "      \"\"\"\n",
    "\n",
    "      file = directory + extension\n",
    "\n",
    "      data = self.unpickle_file(file)\n",
    "\n",
    "      X, Y, y = self.preprocess_data(data, num_of_classes)\n",
    "\n",
    "      return X, Y, y\n",
    "\n",
    "\n",
    "  def unpickle_file(self, filename):\n",
    "\n",
    "      # serialise the object before writing it to file:\n",
    "      # convert it into a character stream\n",
    "      with open(filename, 'rb') as file_opened:\n",
    "          file_data = pickle.load(file_opened, encoding='bytes')\n",
    "\n",
    "      return file_data\n",
    "\n",
    "\n",
    "  def preprocess_data(self, file_data, num_of_classes):\n",
    "      \"\"\"\n",
    "          Args:\n",
    "\n",
    "              param file_data: normalizing between [0,1]\n",
    "              X_data: contains the image pixel data which is d x N = (3072, 10000)\n",
    "              y: vector of length N = 10.000 containing the label for each image\n",
    "              Y is KxN (K = 10) and contains the one-hot representation of the label for each image\n",
    "\n",
    "          Return:\n",
    "\n",
    "              data and labels\n",
    "      \"\"\"\n",
    "\n",
    "      # normalization with respect to the mean and std\n",
    "      # 1 column in X is 1 image, so mean and std is per column\n",
    "      X_data = (file_data[b\"data\"]).T\n",
    "      X_mean = np.mean(X_data, axis=1, keepdims=True)\n",
    "      X_stdev = np.std(X_data, axis=1, keepdims=True)\n",
    "\n",
    "      X_data = (X_data - X_mean) / X_stdev\n",
    "\n",
    "      # the label for each image - vector of length N\n",
    "      y = np.asarray(file_data[b\"labels\"])\n",
    "\n",
    "      # one-hot representation of the label for each image\n",
    "      Y = (np.eye(num_of_classes)[y]).T\n",
    "\n",
    "      return X_data, Y, y\n",
    "\n",
    "  def build_dataset(self):\n",
    "      X_train, Y_train, y_train = self.load_batch(directory, \"/data_batch_1\")\n",
    "      X_val, Y_val, y_val = self.load_batch(directory, \"/data_batch_2\")\n",
    "      X_test, Y_test, y_test = self.load_batch(directory, \"/test_batch\")\n",
    "\n",
    "      print(\"X_train shape = \", X_train.shape)\n",
    "      print(\"Y_train shape = \", Y_train.shape)\n",
    "\n",
    "      print(\"X_val shape = \", X_val.shape)\n",
    "      print(\"Y_val shape = \", Y_val.shape)\n",
    "\n",
    "      print(\"X_test shape = \", X_test.shape)\n",
    "      print(\"Y_test shape = \", Y_test.shape)\n",
    "\n",
    "      return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test\n",
    "\n",
    "  def train_one_batch(self):\n",
    "      \"\"\"\n",
    "          Build datasets where the training data consists of 10,000 images.\n",
    "\n",
    "          Returns:\n",
    "              all the separate data sets and labels (list) with correct image labels\n",
    "      \"\"\"\n",
    "      X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test = self.build_dataset()\n",
    "\n",
    "      labels = self.unpickle_file(file_labels)[b'label_names']\n",
    "\n",
    "      return X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, labels\n",
    "\n",
    "  def find_parameters(self, X, Y):\n",
    "      K = np.shape(Y)[0]\n",
    "      D = np.shape(X)[0]\n",
    "      N = np.shape(X)[1]\n",
    "\n",
    "      labels = self.unpickle_file(file_labels)[b'label_names']\n",
    "\n",
    "      C = len(labels)\n",
    "\n",
    "      return K, D, N, C, labels\n",
    "\n",
    "  def make_layers(self, shapes, activations):\n",
    "      \"\"\"Create the layers of the network\n",
    "      Args:\n",
    "          shapes      (list): the shapes per layer as tuples\n",
    "          activations (list): the activation functions per layer as strings\n",
    "      Returns:\n",
    "          layers (OrderedDict): specifies the shape and activation function of\n",
    "          each layer\n",
    "      \"\"\"\n",
    "      if len(shapes) != len(activations):\n",
    "          raise RuntimeError('The size of shapes should equal the size of activations.')\n",
    "\n",
    "      layers = OrderedDict([])\n",
    "\n",
    "      for i, (shape, activation) in enumerate(zip(shapes, activations)):\n",
    "          layers[\"layer%s\" % i] = {\"shape\": shape, \"activation\": activation}\n",
    "          print(\"Layer \", i, \" = \" , {\"shape\": shape, \"activation\": activation})        \n",
    "\n",
    "      return layers      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwF3ZtQJdwIy"
   },
   "outputs": [],
   "source": [
    "%pycat build_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ReQAMnrudyTg",
    "outputId": "cc38098e-4bc7-4dfa-d877-dbc934f4da96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_tests.py\n",
    "\n",
    "__author__ = \"Xenia Ioannidou\"\n",
    "\n",
    "import statistics\n",
    "\n",
    "from k_layer_network import NeuralNetwork\n",
    "from preprocessor import Preprocessor\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "file_labels = \"//content//drive//My Drive//Colab_Deep_Learning_Lab3//batches.meta\"\n",
    "\n",
    "class Tests():\n",
    "\n",
    "    def exercise_1a(self):\n",
    "      print(\"Checking gradients for 3-layers network\")\n",
    "      shapes=[(50, 30), (50, 50), (10, 50)]\n",
    "      acts=[\"relu\", \"relu\", \"softmax\"]\n",
    "      self.checking_grads_with_BN(shapes, acts)\n",
    "    \n",
    "    def exercise_1b(self):\n",
    "      print(\"\\nChecking gradients for 9-layers network\")\n",
    "      shapes=[(50, 30), (30, 50), (20, 30), (20, 20), (10, 20),\n",
    "                  (10, 10), (10, 10), (10, 10), (10, 10)]\n",
    "      acts=[\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\",\n",
    "                  \"relu\", \"relu\", \"softmax\"]\n",
    "      self.checking_grads_with_BN(shapes, acts)\n",
    "\n",
    "    def exercise_2(self):\n",
    "      shapes=[(50, 3072), (50, 50), (10, 50)]\n",
    "      activations=[\"relu\", \"relu\", \"softmax\"]\n",
    "\n",
    "      # self.training(shapes, activations, batch_norm=False) \n",
    "      self.training(shapes, activations, batch_norm=True) \n",
    "\n",
    "\n",
    "    def exercise_3(self):\n",
    "      shapes=[(50, 3072), (30, 50), (20, 30), (20, 20), (10, 20),(10, 10)]\n",
    "      activations=[\"relu\", \"relu\", \"relu\",\"relu\", \"relu\", \"softmax\"]\n",
    "\n",
    "      self.training(shapes, activations, batch_norm=False) \n",
    "      self.training(shapes, activations, batch_norm=True)\n",
    "    \n",
    "    def check_sensitivity(self, with_BN, std, plt_id):\n",
    "\n",
    "      if with_BN:\n",
    "        print(\"START CHECKING SENSITIVITY TO INITIALIZATION WITH BN\")\n",
    "      else:\n",
    "        print(\"START CHECKING SENSITIVITY TO INITIALIZATION WITHOUT BN\")\n",
    "\n",
    "      # Build dataset with one batch\n",
    "      preprocessor = Preprocessor()\n",
    "      shapes=[(50, 3072), (50, 50), (10, 50)]\n",
    "      activations=[\"relu\", \"relu\", \"softmax\"]\n",
    "      X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, labels = preprocessor.train_one_batch()\n",
    "      layers = preprocessor.make_layers(shapes=shapes, activations=activations)\n",
    "      random.shuffle(X_train)\n",
    "      random.shuffle(X_val)\n",
    "      random.shuffle(X_test)\n",
    "      K, D, N, C, labels = preprocessor.find_parameters(X_train[:30, :5], Y_train[:30, :5])\n",
    "      \n",
    "      # Build network\n",
    "      net = NeuralNetwork(K, D, N, C, labels, layers, std=std, batch_norm=with_BN)\n",
    "\n",
    "      # Train network\n",
    "      acc_train, acc_val, acc_test = net.mini_batch_gd(\n",
    "                  X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, \n",
    "                  Y_test,y_test,labda=0.021,batch_s=100,learning_rate_min=1e-5, \n",
    "                  learning_rate_max=1e-1,stepsize=2250,n_epochs=20,\n",
    "                  plot_id=plt_id, verbose=True)\n",
    "\n",
    "    def coarse_search(self):\n",
    "      print(\"STARTING COARSE SEARCH\")\n",
    "\n",
    "      # Build dataset with one batch\n",
    "      preprocessor = Preprocessor()\n",
    "      shapes=[(50, 3072), (50, 50), (10, 50)]\n",
    "      activations=[\"relu\", \"relu\", \"softmax\"]\n",
    "      X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, labels = preprocessor.train_one_batch()\n",
    "      layers = preprocessor.make_layers(shapes=shapes, activations=activations)\n",
    "      K, D, N, C, labels = preprocessor.find_parameters(X_train[:30, :5], Y_train[:30, :5])\n",
    "      \n",
    "      # Build network\n",
    "      net = NeuralNetwork(K, D, N, C, labels, layers, batch_norm=True)\n",
    "\n",
    "      # Coarse random search of the lambda\n",
    "      lamda_list = []\n",
    "      while len(lamda_list) < 30:\n",
    "          x = np.random.uniform(1e-5, 1e-1)\n",
    "          lamda_list.append(x)\n",
    "      \n",
    "      for lamda_i in lamda_list:\n",
    "            print(\"Lambda = \", lamda_i)\n",
    "\n",
    "            # Train network\n",
    "            acc_train, acc_val, acc_test = net.mini_batch_gd(\n",
    "                  X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, \n",
    "                  Y_test,y_test,labda=lamda_i,batch_s=100,learning_rate_min=1e-5, \n",
    "                  learning_rate_max=1e-1,stepsize=2250,n_epochs=20,verbose=True)\n",
    "\n",
    "            print(\"___________________________\")\n",
    "\n",
    "    def fine_search(self):\n",
    "      print(\"STARTING FINE SEARCH\")\n",
    "\n",
    "      # Build dataset with one batch\n",
    "      preprocessor = Preprocessor()\n",
    "      shapes=[(50, 3072), (50, 50), (10, 50)]\n",
    "      activations=[\"relu\", \"relu\", \"softmax\"]\n",
    "      X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, labels = preprocessor.train_one_batch()\n",
    "      layers = preprocessor.make_layers(shapes=shapes, activations=activations)\n",
    "      K, D, N, C, labels = preprocessor.find_parameters(X_train[:30, :5], Y_train[:30, :5])\n",
    "      \n",
    "      # Build network\n",
    "      net = NeuralNetwork(K, D, N, C, labels, layers, batch_norm=True)\n",
    "\n",
    "      # Coarse random search of the lambda\n",
    "      lamda_list = []\n",
    "      while len(lamda_list) < 30:\n",
    "          x = np.random.uniform(0.01, 0.1)\n",
    "          lamda_list.append(x)\n",
    "      \n",
    "      for lamda_i in lamda_list:\n",
    "            print(\"Lambda = \", lamda_i)\n",
    "\n",
    "            # Train network\n",
    "            acc_train, acc_val, acc_test = net.mini_batch_gd(\n",
    "                  X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, \n",
    "                  Y_test,y_test,labda=lamda_i,batch_s=100,learning_rate_min=1e-5, \n",
    "                  learning_rate_max=1e-1,stepsize=2250,n_epochs=10, verbose=True)\n",
    "\n",
    "            print(\"___________________________\")\n",
    "\n",
    "\n",
    "    def checking_grads_with_BN(self, shapes, acts):\n",
    "      preprocessor = Preprocessor()\n",
    "      X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, labels = preprocessor.train_one_batch()\n",
    "      layers = preprocessor.make_layers(shapes=shapes, activations=acts)\n",
    "      K, D, N, C, labels = preprocessor.find_parameters(X_train[:30, :5], Y_train[:30, :5])\n",
    "      \n",
    "      net = NeuralNetwork(K, D, N, C, labels, layers, batch_norm=True)\n",
    "\n",
    "      grads_ana = net.compute_gradients(X_train[:30, :5], Y_train[:30, :5],\n",
    "                                        labda = 0)\n",
    "\n",
    "      grads_num = net.compute_gradients_num(X_train[:30, :5], Y_train[:30, :5],\n",
    "                                          labda=0)\n",
    "\n",
    "      net.check_gradients(grads_ana, grads_num)\n",
    "\n",
    "    def training(self, shapes, activations, batch_norm, labda=0.021):\n",
    "        \"\"\"Train a model\"\"\"\n",
    "\n",
    "        preprocessor = Preprocessor()\n",
    "        X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, Y_test, y_test, labels = preprocessor.train_one_batch()\n",
    "        layers = preprocessor.make_layers(shapes=shapes, activations=activations)\n",
    "        K, D, N, C, labels = preprocessor.find_parameters(X_train[:30, :5], Y_train[:30, :5])\n",
    "\n",
    "        acc_train_set = []\n",
    "        acc_val_set = []\n",
    "        acc_test_set = []\n",
    "        for j in range(10):\n",
    "          print(\"\\nLoop: \", j)\n",
    "          net = NeuralNetwork(K, D, N, C, labels, layers, batch_norm=batch_norm)\n",
    "          acc_train, acc_val, acc_test = net.mini_batch_gd(\n",
    "                  X_train, Y_train, y_train, X_val, Y_val, y_val, X_test, \n",
    "                  Y_test, y_test, labda=labda, batch_s=100, learning_rate_min=1e-5, \n",
    "                  learning_rate_max=1e-1, stepsize=2250, n_epochs=10, verbose=True)\n",
    "\n",
    "          acc_train_set.append(acc_train)\n",
    "          acc_val_set.append(acc_val)\n",
    "          acc_test_set.append(acc_test)\n",
    "\n",
    "        print(\"\\nTrain mean accuracy:\" + str(statistics.mean(acc_train_set)))\n",
    "        print(\"Val mean accuracy:\" + str(statistics.mean(acc_val_set)))\n",
    "        print(\"Test mean accuracy:\" + str(statistics.mean(acc_test_set)))\n",
    "        print(\"Train stdev accuracy:\" + str(statistics.stdev(acc_train_set)))\n",
    "        print(\"Val stdev accuracy:\" + str(statistics.stdev(acc_val_set)))\n",
    "        print(\"Test stdev accuracy:\" + str(statistics.stdev(acc_test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIdzGOA9fCD8"
   },
   "outputs": [],
   "source": [
    "%pycat k_layer_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_00WNwPRfGWw",
    "outputId": "51574ebb-77f0-437c-de73-0e8cc59c57fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting k_layer_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile k_layer_network.py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import unittest\n",
    "import statistics\n",
    "import re\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import scipy.misc\n",
    "\n",
    "np.seterr(all='warn')\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self,K,D,N,C,labels,layers,std=1e-1,alpha=0.8,batch_norm=True):\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.N = N\n",
    "        self.C = C\n",
    "        self.labels     = labels\n",
    "        self.layers     = layers\n",
    "        self.k          = len(layers) - 1\n",
    "        self.alpha      = alpha\n",
    "        self.batch_norm = batch_norm\n",
    "        self.mu_av, self.var_av = [], []\n",
    "\n",
    "        if self.batch_norm:\n",
    "          print(\"Running Model with Batch normalization\")\n",
    "        else:\n",
    "          print(\"Running Model without Batch normalization\")\n",
    "\n",
    "        self.activation_funcs = {'relu': self.compute_relu, 'softmax': self.compute_softmax}\n",
    "\n",
    "        self.W, self.b, self.gamma, self.beta = [], [], [], []\n",
    "        self.activations = []\n",
    "\n",
    "        for layer in layers.values():\n",
    "            for key, val in layer.items():\n",
    "                if key == \"shape\":\n",
    "                    W      = np.random.normal(0, std, size=(val[0], val[1]))\n",
    "                    b      = np.zeros(val[0]).reshape(val[0], 1)\n",
    "                    gamma  = np.ones((val[0], 1))\n",
    "                    beta   = np.zeros((val[0], 1))\n",
    "                    mu_av  = np.zeros((val[0], 1))\n",
    "                    var_av = np.zeros((val[0], 1))\n",
    "                    self.W.append(W)\n",
    "                    self.b.append(b)\n",
    "                    self.gamma.append(gamma)\n",
    "                    self.beta.append(beta)\n",
    "                    self.mu_av.append(mu_av)\n",
    "                    self.var_av.append(var_av)\n",
    "                elif key == \"activation\":\n",
    "                    self.activations.append((val, self.activation_funcs[val]))\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.params = {\"W\": self.W, \"b\": self.b, \"gamma\": self.gamma,\n",
    "                    \"beta\": self.beta}\n",
    "        else:\n",
    "            self.params = {\"W\": self.W, \"b\": self.b}\n",
    "\n",
    "\n",
    "    def compute_softmax(self, x):\n",
    "        e = x - np.max(x)\n",
    "        return np.exp(e) / np.sum(np.exp(e), axis=0)\n",
    "\n",
    "\n",
    "    def compute_relu(self, x):\n",
    "        result = np.maximum(x, 0)\n",
    "        return result\n",
    "\n",
    "    def batch_normalize(self, s, mu, var, epsilon):\n",
    "      s_norm = np.zeros(np.shape(s))\n",
    "\n",
    "      a = var + epsilon\n",
    "      a = np.power(a, (-1 / 2))\n",
    "      a = np.diag(a)\n",
    "\n",
    "      s_norm = (s - mu) / np.sqrt(a)\n",
    "\n",
    "      return s_norm\n",
    "    \n",
    "    def compute_ce_loss(self, Y, P):\n",
    "      \"\"\"\n",
    "          compute cross entropy loss\n",
    "      \"\"\"\n",
    "\n",
    "      p_one_hot = np.sum(np.prod((np.array(Y), P), axis=0), axis=0)\n",
    "      loss = np.sum(0 - np.log(p_one_hot))\n",
    "\n",
    "      return loss\n",
    "\n",
    "    def compute_cost(self, X, Y, labda):\n",
    "      \"\"\"Find the cost of the neural network\n",
    "\n",
    "            X    : array with (D, N) dimensions\n",
    "            Y    : one-hot encoding labels array (C, N)\n",
    "            lamda: the regularization term\n",
    "      \"\"\"\n",
    "      n = X.shape[1] # number of images\n",
    "\n",
    "      if self.batch_norm:\n",
    "        H, P, S, S_hat, means, variances = self.evaluate_classifier(X)\n",
    "      else:\n",
    "        H, P = self.evaluate_classifier(X)\n",
    "\n",
    "      # cross entropy loss function\n",
    "      loss = self.compute_ce_loss(Y, P) / n\n",
    "\n",
    "      # regularization term\n",
    "      regularization = 0\n",
    "      for W_i in self.W:\n",
    "          regularization += (np.sum(np.square(W_i)))\n",
    "\n",
    "      # compute cost\n",
    "      cost = loss + labda * regularization\n",
    "\n",
    "      return cost, loss\n",
    "\n",
    "\n",
    "    def compute_accuracy(self, X, y, is_testing=False):\n",
    "        \"\"\" Computes the accuracy of the classifier\"\"\"\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            P = self.evaluate_classifier(X, is_testing=is_testing)\n",
    "            predictions = np.argmax(P[1], axis=0)\n",
    "        else:\n",
    "            P = self.evaluate_classifier(X)\n",
    "            predictions = np.argmax(P[1], axis=0)\n",
    "        \n",
    "        corrects = np.where(predictions - np.asarray(y) == 0)\n",
    "        num_of_corrects = len(corrects[0])\n",
    "        accuracy = num_of_corrects / X.shape[1]\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def evaluate_classifier(self, X, is_testing=False, is_training=True):\n",
    "        N = X.shape[1]\n",
    "        s = np.copy(X)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            # Run Forward Pass of Batch Normalization\n",
    "            H, P, S, S_hat, means, variances = self.forward_BN(X, is_training, is_testing)\n",
    "            return H, P, S, S_hat, means, variances\n",
    "        else:\n",
    "            H = []\n",
    "            P = []\n",
    "            layer_counter = 1\n",
    "            last_layer = len(self.layers)\n",
    "\n",
    "            for W, b in zip(self.W, self.b):\n",
    "                if layer_counter != last_layer:\n",
    "                    s = self.compute_relu(np.dot(W,s) + b)\n",
    "                    H.append(s)\n",
    "                else:\n",
    "                    P = self.compute_softmax(np.dot(W,s) + b)\n",
    "\n",
    "                layer_counter += 1\n",
    "\n",
    "            return H, P\n",
    "\n",
    "    def forward_BN(self, X, is_training, is_testing):\n",
    "      S, S_hat, means, variances, H = [], [], [], [], []\n",
    "      s=np.copy(X)\n",
    "      layer_counter = 1\n",
    "      last_layer = len(self.layers)\n",
    "\n",
    "      for i, (W, b, gamma, beta, mu_av, var_av) in enumerate(\n",
    "              zip(self.W, self.b, self.gamma, self.beta, self.mu_av,\n",
    "                  self.var_av)):\n",
    "\n",
    "          H.append(s)\n",
    "          s = np.dot(W,s) + b\n",
    "\n",
    "          if layer_counter < last_layer:\n",
    "              S.append(s)\n",
    "              if is_testing:\n",
    "                  s = (s - mu_av) / np.sqrt(var_av + \\\n",
    "                          np.finfo(np.float64).eps)\n",
    "\n",
    "              else:\n",
    "                  mu = np.mean(s, axis=1, keepdims=True)\n",
    "                  means.append(mu)\n",
    "                  var = np.var(s, axis=1, keepdims=True) * (self.N-1)/self.N\n",
    "                  variances.append(var)\n",
    "\n",
    "                  if is_training:\n",
    "                      self.mu_av[i]  = self.alpha * mu_av + \\\n",
    "                              (1-self.alpha) * mu\n",
    "                      self.var_av[i] = self.alpha * var_av + \\\n",
    "                              (1-self.alpha) * var\n",
    "\n",
    "                  s = self.batch_normalize( s, mu, var, np.finfo(np.float64).eps)\n",
    "\n",
    "              S_hat.append(s)\n",
    "              s = self.compute_relu(np.multiply(gamma, s) + beta)\n",
    "\n",
    "          else:\n",
    "              P = self.compute_softmax(s)\n",
    "          \n",
    "          layer_counter += 1\n",
    "\n",
    "      return H, P, S, S_hat, means, variances\n",
    "    \n",
    "    def backward_BN(self, Y_batch, H_batch, P_batch, S_batch, labda, S_hat_batch,\n",
    "                    means_batch, vars_batch, N, grads):\n",
    "      e = np.finfo(np.float64).eps\n",
    "      last_layer = len(self.layers) - 1\n",
    "\n",
    "      G_batch = - (Y_batch - P_batch)\n",
    "\n",
    "      # For the last layer\n",
    "      grads[\"W\"][last_layer] = 1/N * np.dot(G_batch,H_batch[last_layer].T) + \\\n",
    "                    2 * labda * self.W[last_layer]\n",
    "      grads[\"b\"][last_layer] = np.reshape(1/N * np.dot(G_batch,np.ones(N)),\n",
    "              (grads[\"b\"][last_layer].shape[0], 1))\n",
    "      \n",
    "      G_batch = np.dot(self.W[last_layer].T, G_batch)\n",
    "      H_batch[last_layer][H_batch[last_layer] <= 0] = 0\n",
    "      G_batch = np.multiply(G_batch, H_batch[last_layer] > 0)\n",
    "\n",
    "      # Loop hidden layers\n",
    "      for l in range(self.k - 1, -1, -1):\n",
    "          grads[\"gamma\"][l] = np.reshape(1/N * np.dot(np.multiply(G_batch,\n",
    "              S_hat_batch[l]),np.ones(N)), (grads[\"gamma\"][l].shape[0], 1))\n",
    "          grads[\"beta\"][l]  = np.reshape(1/N * np.dot(G_batch, np.ones(N)),\n",
    "                  (grads[\"beta\"][l].shape[0], 1))\n",
    "\n",
    "          G_batch = np.multiply(G_batch, self.gamma[l])\n",
    "\n",
    "          G_batch = self.batch_norm_back_pass(G_batch, S_batch[l],\n",
    "                  means_batch[l], vars_batch[l], e)\n",
    "\n",
    "          grads[\"W\"][l] = 1/N * np.dot(G_batch,H_batch[l].T) + 2 * labda * self.W[l]\n",
    "\n",
    "          grads[\"b\"][l] = np.reshape(1/N * np.dot(G_batch, np.ones(N)),\n",
    "                                  (grads[\"b\"][l].shape[0], 1))\n",
    "          if l > 0:\n",
    "              G_batch = np.dot(self.W[l].T, G_batch)\n",
    "              H_batch[l][H_batch[l] <= 0] = 0\n",
    "              G_batch = np.multiply(G_batch, H_batch[l] > 0)\n",
    "\n",
    "      return grads\n",
    "\n",
    "    def compute_gradients(self, X_batch, Y_batch, labda):\n",
    "        \"\"\"Analytically computes the gradients of the weight and bias parameters\n",
    "        Args:\n",
    "            X_batch (np.ndarray): data batch matrix (D, N)\n",
    "            Y_batch (np.ndarray): one-hot-encoding labels batch vector (C, N)\n",
    "            labda        (float): regularization term\n",
    "        Returns:\n",
    "            grads (dict): the updated analytical gradients\n",
    "        \"\"\"\n",
    "        N = X_batch.shape[1]\n",
    "        e = np.finfo(np.float64).eps\n",
    "\n",
    "        if self.batch_norm:\n",
    "            grads = {\"W\": [], \"b\": [], \"gamma\": [], \"beta\": []}\n",
    "\n",
    "            # Initialize networks parameters\n",
    "            for key in self.params:\n",
    "                for par in self.params[key]:\n",
    "                    grads[key].append(np.zeros_like(par))\n",
    "\n",
    "            # Forward pass\n",
    "            H_batch, P_batch, S_batch, S_hat_batch, means_batch, vars_batch = \\\n",
    "                    self.evaluate_classifier(X_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            grads = self.backward_BN(Y_batch, H_batch, P_batch, S_batch, labda, \\\n",
    "                                S_hat_batch, means_batch, vars_batch, N, grads)\n",
    "\n",
    "        else:\n",
    "            grads = {\"W\": [], \"b\": []}\n",
    "            for W, b in zip(self.W, self.b):\n",
    "                grads[\"W\"].append(np.zeros_like(W))\n",
    "                grads[\"b\"].append(np.zeros_like(b))\n",
    "\n",
    "            # Forward pass\n",
    "            H_batch, P_batch = self.evaluate_classifier(X_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            G_batch = - (Y_batch - P_batch)\n",
    "\n",
    "            # Loop layers\n",
    "            for l in range(len(self.layers) - 1, 0, -1):\n",
    "                grads[\"W\"][l] = 1/N * np.dot(G_batch, H_batch[l-1].T) \\\n",
    "                                    + 2 * labda * self.W[l]\n",
    "                grads[\"b\"][l] = np.reshape(1/N * np.dot(G_batch, np.ones(N)),\n",
    "                        (grads[\"b\"][l].shape[0], 1))\n",
    "\n",
    "                G_batch = np.dot(self.W[l].T, G_batch)\n",
    "                H_batch[l-1][H_batch[l-1] <= 0] = 0\n",
    "                G_batch = np.multiply(G_batch, H_batch[l-1] > 0)\n",
    "\n",
    "            grads[\"W\"][0] = ((np.dot(G_batch, np.transpose(X_batch))) / N) \\\n",
    "                              + labda * self.W[0]\n",
    "            grads[\"b\"][0] = np.reshape(np.asarray((np.dot(G_batch, np.ones(N))))/N,\\\n",
    "                                       self.b[0].shape)\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def batch_norm_back_pass(self, G_batch, S_batch, mean_batch, var_batch, e):\n",
    "        \"\"\"Computation of the batch normalization back pass\n",
    "        Args:\n",
    "            G_batch    : gradients of the batch\n",
    "            S_batch    : linear transformations of the batch\n",
    "            mean_batch : mean vectors of the batch\n",
    "            var_bath   : variance vectors of the batch\n",
    "        Returns:\n",
    "            G_batch : batch normalized gradients\n",
    "        \"\"\"\n",
    "        N = G_batch.shape[1]\n",
    "        sigma1 = np.power(var_batch + e, -0.5) \n",
    "        sigma2 = np.power(var_batch + e, -1.5) \n",
    "\n",
    "        G1 = np.multiply(G_batch, sigma1)\n",
    "        G2 = np.multiply(G_batch, sigma2)\n",
    "\n",
    "        D = S_batch - mean_batch\n",
    "\n",
    "        c = np.sum(np.multiply(G2, D), axis=1, keepdims=True)\n",
    "\n",
    "        G_batch = G1 - 1/N * np.sum(G1, axis=1, keepdims=True) - \\\n",
    "                1/N * np.multiply(D, c)\n",
    "\n",
    "        return G_batch\n",
    "\n",
    "\n",
    "    def compute_gradients_num(self, X_batch, Y_batch, size=2,\n",
    "            labda=np.float64(0), h=np.float64(1e-7)):\n",
    "        \"\"\"Numerically computes the gradients of the weight and bias parameters\n",
    "        Args:\n",
    "            X_batch : data batch matrix (D, N)\n",
    "            Y_batch : one-hot-encoding labels batch vector (C, N)\n",
    "            W       : the weight matrix\n",
    "            b       : the bias matrix\n",
    "            labda   : penalty term\n",
    "            h       : marginal offset\n",
    "        Returns:\n",
    "            grads  (dict): the numerically gradients\n",
    "        \"\"\"\n",
    "        if self.batch_norm:\n",
    "            grads = {\"W\": [], \"b\": [], \"gamma\": [], \"beta\": []}\n",
    "        else:\n",
    "            grads = {\"W\": [], \"b\": []}\n",
    "\n",
    "        for j in range(len(self.b)):\n",
    "            for key in self.params:\n",
    "                grads[key].append(np.zeros(self.params[key][j].shape))\n",
    "                for i in range(len(self.params[key][j].flatten())):\n",
    "                    old_par = self.params[key][j].flat[i]\n",
    "                    self.params[key][j].flat[i] = old_par + h\n",
    "                    _, c2 = self.compute_cost(X_batch, Y_batch, labda)\n",
    "                    self.params[key][j].flat[i] = old_par - h\n",
    "                    _, c3 = self.compute_cost(X_batch, Y_batch, labda)\n",
    "                    self.params[key][j].flat[i] = old_par\n",
    "                    grads[key][j].flat[i] = (c2-c3) / (2*h)\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def check_gradients(self, grads_analyt, grads_num):\n",
    "        \"\"\"Compute the relative error between analytical and numerical grads\"\"\"\n",
    "\n",
    "        layers = len(self.layers)\n",
    "\n",
    "        for l in range(layers):\n",
    "            for key in grads_analyt:\n",
    "                num = abs(grads_analyt[key][l].flat[:] - grads_num[key][l].flat[:])\n",
    "                denominator = np.asarray([max(abs(a), abs(b)) + 1e-10 for a,b in\n",
    "                    zip(grads_analyt[key][l].flat[:], grads_num[key][l].flat[:])])\n",
    "                max_rel_err = max(num / denominator)\n",
    "                print(\"The relative error for layer %d %s: %.6g\" %\n",
    "                        (l+1, key, max_rel_err))\n",
    "\n",
    "\n",
    "    def visualization_per_epoch(self, n_epochs,arg1,arg2,title,y_label,min,max):\n",
    "       \"\"\" Plots \n",
    "        \n",
    "            n_epochs       (int): number of training epochs\n",
    "            i_train (np.ndarray): input to plot per epoch on the training set\n",
    "            i_val   (np.ndarray): input to plot per epoch on the validation set\n",
    "            title          (str): plot title\n",
    "            y_label        (str): y-axis label\n",
    "       \"\"\"\n",
    "       epochs = np.arange(n_epochs)\n",
    "\n",
    "       fig, ax = plt.subplots(figsize=(10, 8))\n",
    "       ax.plot(epochs, arg1, label=\"Training set\")\n",
    "       ax.plot(epochs, arg2, label=\"Validation set\")\n",
    "       ax.legend()\n",
    "       ax.set(xlabel='Number of epochs', ylabel = y_label)\n",
    "       ax.set_ylim([min, max])\n",
    "      #  ax.set_xlim([0, 8])\n",
    "       ax.grid()\n",
    "\n",
    "       plt.savefig(\"plots/\" + title + \".png\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "    def cyclical_learning_rate(self, learning_rate, stepsize, learning_rate_min, learning_rate_max, t):\n",
    "\n",
    "        if t <= stepsize:\n",
    "            learning_rate = learning_rate_min + t / stepsize * (learning_rate_max - learning_rate_min)\n",
    "\n",
    "        elif t <= 2 * stepsize:\n",
    "            learning_rate = learning_rate_max - (t - stepsize) / stepsize * (\n",
    "                        learning_rate_max - learning_rate_min)\n",
    "\n",
    "        return learning_rate\n",
    "\n",
    "\n",
    "    def mini_batch_gd(self, X, Y, y, X_val, Y_val, y_val, X_test, Y_test, y_test, \n",
    "                      labda=0, batch_s=100, learning_rate_min=1e-5, \n",
    "                      learning_rate_max=1e-1, stepsize=800, n_epochs=40, \n",
    "                      plot_id=\"\", verbose=True, plot=True,is_testing=True, is_training=True):\n",
    "      \"\"\"\n",
    "            TRAIN the model using mini-batch gradient descent\n",
    "      \"\"\"\n",
    "    \n",
    "      train_cost = []\n",
    "      train_loss = []\n",
    "      train_accuracy = []\n",
    "      val_cost = []\n",
    "      val_loss = []\n",
    "      val_accuracy = []\n",
    "      test_cost = []\n",
    "      test_loss = []\n",
    "      test_accuracy = []\n",
    "\n",
    "      n_batch = int(np.floor(X.shape[1] / batch_s))\n",
    "      learning_rate_current = learning_rate_min\n",
    "      N = np.shape(X)[1]\n",
    "      images_per_batch = int(N / n_batch)\n",
    "      t = 0\n",
    "\n",
    "      #  Generate the set of mini - batches and do the Gradient Descent\n",
    "      for epoch in range(n_epochs):\n",
    "        for batch in range(n_batch):\n",
    "              j_start = (batch) * images_per_batch\n",
    "              j_end = (batch+1) * images_per_batch\n",
    "\n",
    "              X_batch = X[:, j_start:j_end]\n",
    "              Y_batch = Y[:, j_start:j_end]\n",
    "\n",
    "              grads = self.compute_gradients(X_batch, Y_batch, labda)\n",
    "\n",
    "              for key in self.params:\n",
    "                  for par, grad in zip(self.params[key], grads[key]):\n",
    "                      par -= learning_rate_current * grad\n",
    "\n",
    "              # Apply cyclical learning rates\n",
    "              learning_rate_current = self.cyclical_learning_rate(learning_rate_current,\n",
    "                            stepsize, learning_rate_min, learning_rate_max, t)\n",
    "\n",
    "              t = (t+1) % (2*stepsize)\n",
    "\n",
    "        # TRAINING\n",
    "        # compute accuracy per epoch and save the results in list\n",
    "        epoch_accuracy = self.compute_accuracy(X, y)\n",
    "        train_accuracy.append(epoch_accuracy)\n",
    "\n",
    "        # compute cost per epoch and save the results in list\n",
    "        epoch_cost, epoch_loss = self.compute_cost(X, Y, labda)\n",
    "        train_cost.append(epoch_cost)\n",
    "        train_loss.append(epoch_loss)\n",
    "\n",
    "        # CROSS VALIDATION\n",
    "        # compute accuracy per epoch and save the results in list\n",
    "        val_epoch_accuracy = self.compute_accuracy(X_val, y_val)\n",
    "        val_accuracy.append(val_epoch_accuracy)\n",
    "\n",
    "        # compute cost per epoch and save the results in list\n",
    "        val_epoch_cost, val_epoch_loss = self.compute_cost(X_val, Y_val, labda)\n",
    "        val_cost.append(val_epoch_cost)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "\n",
    "        # TEST\n",
    "        # compute accuracy per epoch and save the results in list\n",
    "        test_epoch_accuracy = self.compute_accuracy(X_test, y_test)\n",
    "        test_accuracy.append(test_epoch_accuracy)\n",
    "\n",
    "        # compute cost per epoch and save the results in list\n",
    "        test_epoch_cost, test_epoch_loss = self.compute_cost(X_val, Y_val, labda)\n",
    "        test_cost.append(test_epoch_cost)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "\n",
    "      print(\"Validation cost = \", val_cost)\n",
    "      print(\"Validation loss = \", val_loss )\n",
    "      print(\"Training cost = \", train_cost)\n",
    "      print(\"Training loss = \", train_loss )\n",
    "\n",
    "      if plot:\n",
    "          self.visualization_per_epoch(n_epochs, train_cost, val_cost, plot_id + \"_cost_plot\",\n",
    "                    y_label=\"Cost\", min = 0, max = 12)\n",
    "          \n",
    "          self.visualization_per_epoch(n_epochs, train_loss, val_loss, plot_id + \"_loss_plot\",\n",
    "                    y_label=\"Loss\", min = 0, max = 4)\n",
    "          \n",
    "\n",
    "      acc_train = self.compute_accuracy(X, y)\n",
    "      acc_val   = self.compute_accuracy(X_val, y_val)\n",
    "      acc_test  = self.compute_accuracy(X_test, y_test, is_testing=True)\n",
    "\n",
    "      if verbose:\n",
    "          print(\"The accuracy on the training set is: \"   + str(acc_train))\n",
    "          print(\"The accuracy on the validation set is: \" + str(acc_val))\n",
    "          print(\"The accuracy on the testing set is: \"    + str(acc_test))\n",
    "\n",
    "      return acc_train, acc_val, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "colab_type": "code",
    "id": "4LDNBoH6Lcsf",
    "outputId": "eb2887f2-4c72-4401-bc6e-c221be97923a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START CHECKING SENSITIVITY TO INITIALIZATION WITH BN\n",
      "X_train shape =  (3072, 10000)\n",
      "Y_train shape =  (10, 10000)\n",
      "X_val shape =  (3072, 10000)\n",
      "Y_val shape =  (10, 10000)\n",
      "X_test shape =  (3072, 10000)\n",
      "Y_test shape =  (10, 10000)\n",
      "Layer  0  =  {'shape': (50, 3072), 'activation': 'relu'}\n",
      "Layer  1  =  {'shape': (50, 50), 'activation': 'relu'}\n",
      "Layer  2  =  {'shape': (10, 50), 'activation': 'softmax'}\n",
      "Running Model with Batch normalization\n",
      "Validation cost =  [2.2728257836058052, 2.28020441010398, 2.298484302052044, 2.329112465485785, 2.357267598634814, 2.3895846611500584, 2.481161554111369, 2.526067215616488, 2.594899968982956, 2.6200768199251607, 2.722719741505192, 2.709310766961196, 2.781001215149464, 2.760049104779953, 2.72060401334313, 2.6967357868854536, 2.77111106001834, 2.7673877323038703, 2.7592425781126666, 2.757058128532006]\n",
      "Validation loss =  [2.272299020493205, 2.2756439739890966, 2.285645181073142, 2.3047076620179867, 2.316722872088379, 2.32726546814844, 2.392977290308822, 2.4110311627278205, 2.4535782250206024, 2.4548480785226054, 2.535302872597574, 2.503402416710567, 2.5592492834527327, 2.524388264356655, 2.4736886529214956, 2.442866991435788, 2.509885982720838, 2.4992632928025325, 2.4873337315724666, 2.4830845462543594]\n",
      "Training cost =  [2.2024307413054793, 2.091698322749407, 1.9925565699463663, 1.9180295571629977, 1.8473151106183203, 1.8023421788208493, 1.730347784614294, 1.716741579688258, 1.697715312451514, 1.7033897927834112, 1.717765288238423, 1.7005736004220249, 1.7113417115393255, 1.6871354109725698, 1.72021944800419, 1.708631443383546, 1.7104869416066015, 1.7545744124188045, 1.738767023804113, 1.7527782033597907]\n",
      "Training loss =  [2.2019039781928793, 2.0871378866345234, 1.9797174489674645, 1.8936247536951996, 1.8067703840718852, 1.7400229858192309, 1.642163520811747, 1.6017055267995903, 1.5563935684891606, 1.538161051380856, 1.530348419330805, 1.4946652501713955, 1.489589779842594, 1.4514745705492715, 1.4733040875825556, 1.4547626479338804, 1.4492618643091, 1.4864499729174665, 1.4668581772639129, 1.478804621082144]\n",
      "The accuracy on the training set is: 0.4767\n",
      "The accuracy on the validation set is: 0.1684\n",
      "The accuracy on the testing set is: 0.1453\n",
      "START CHECKING SENSITIVITY TO INITIALIZATION WITHOUT BN\n",
      "X_train shape =  (3072, 10000)\n",
      "Y_train shape =  (10, 10000)\n",
      "X_val shape =  (3072, 10000)\n",
      "Y_val shape =  (10, 10000)\n",
      "X_test shape =  (3072, 10000)\n",
      "Y_test shape =  (10, 10000)\n",
      "Layer  0  =  {'shape': (50, 3072), 'activation': 'relu'}\n",
      "Layer  1  =  {'shape': (50, 50), 'activation': 'relu'}\n",
      "Layer  2  =  {'shape': (10, 50), 'activation': 'softmax'}\n",
      "Running Model without Batch normalization\n",
      "Validation cost =  [2.302582684442896, 2.3025789951358524, 2.302580367538222, 2.302592863457635, 2.302619495305752, 2.302658951990866, 2.302706466516518, 2.302755930639664, 2.3028019854411976, 2.3028412398011575, 2.3028724628581614, 2.302896082888259, 2.3029134498219928, 2.302926192843064, 2.3029358103890405, 2.3029434859858906, 2.302950059640249, 2.302956079257663, 2.3029618774148246, 2.302967643339676]\n",
      "Validation loss =  [2.302582358084789, 2.302578677884327, 2.30258006494304, 2.3025925802608658, 2.3026192352256354, 2.3026587175968003, 2.3027062591990384, 2.3027557506683327, 2.3028018320945716, 2.302841111547521, 2.302872357563681, 2.3028959980303427, 2.3029133826891495, 2.302926140707506, 2.3029357706437303, 2.302943456242916, 2.302950037791753, 2.3029560635037507, 2.3029618662647953, 2.302967635593707]\n",
      "Training cost =  [2.302565474805229, 2.3025161577415254, 2.302448924007905, 2.3023776410764483, 2.302313980153659, 2.302264643832059, 2.3022311144113705, 2.302211211440677, 2.3022011844257455, 2.30219734945911, 2.302196909875135, 2.3021980988853827, 2.3021999664375006, 2.302202081348421, 2.3022042888105596, 2.302206557690741, 2.3022089007590862, 2.3022113396880157, 2.302213892777074, 2.3022165724187422]\n",
      "Training loss =  [2.302565148447122, 2.30251584049, 2.302448621412723, 2.302377357879679, 2.3023137200735424, 2.3022644094379934, 2.302230907093891, 2.302211031469346, 2.3022010310791194, 2.3021972212054735, 2.302196804580655, 2.3021980140274665, 2.3021998993046573, 2.302202029212863, 2.3022042490652495, 2.3022065279477664, 2.3022088789105903, 2.3022113239341033, 2.3022138816270448, 2.3022165646727735]\n",
      "The accuracy on the training set is: 0.1032\n",
      "The accuracy on the validation set is: 0.101\n",
      "The accuracy on the testing set is: 0.1\n"
     ]
    }
   ],
   "source": [
    "!python run_tests.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "multi-layer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
